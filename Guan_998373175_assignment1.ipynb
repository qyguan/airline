{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIE1624 Assignment 1\n",
    "\n",
    "Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning\n",
    "\n",
    "Load files from current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = open(\"stop_words.txt\",\"r\")\n",
    "stopWords = stops.read().replace('\\t','\\n').split('\\n')\n",
    "airlineTweets = pd.read_csv(\"US_airline_tweets.csv\")\n",
    "genericTweets = pd.read_csv(\"generic_tweets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data cleaning methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1 - Remove all non ASCII charaters such as 'Ã¥'\n",
    "def non_ascii_rm(text):\n",
    "    allAscii = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    return allAscii\n",
    "    \n",
    "#Method 2 - Remove all html tags and attributes\n",
    "def html_tag_rm(text):\n",
    "    noTag = re.compile(r\"<.*?>\").sub(\"\", text)\n",
    "    return noTag\n",
    "\n",
    "#Method 3 - Replace Html character codes with ASCII equivalent\n",
    "def ascii_rpl(text):\n",
    "    return(html.unescape(text))\n",
    "\n",
    "#Method 4 - Remove all URLs\n",
    "def url_rm(text):\n",
    "    return(re.compile(r\"\\w+://\\S*\").sub(\"\", text))\n",
    "\n",
    "#Method 5 - Remove all stopwords - loaded from stopword.txt\n",
    "def stop_words_rm(stopWords,text):\n",
    "    filtered_words = [word for word in text.split(\" \") if word not in stopWords]\n",
    "    return filtered_words\n",
    "\n",
    "#Wraper function to preform all data cleaning method\n",
    "def cleanText(text):\n",
    "    global stopWords\n",
    "    allAscii = non_ascii_rm(text.lower()) #make sure all lowercase\n",
    "    noTag = html_tag_rm(allAscii) \n",
    "    asciiRpl = ascii_rpl(noTag)\n",
    "    noUrl = url_rm(asciiRpl)\n",
    "    noStopWords = stop_words_rm(stopWords, noUrl)\n",
    "    return(\" \".join(noStopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Data using methods defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandata = lambda x: cleanText(x)\n",
    "genericTweets[\"text\"] = genericTweets[\"text\"].map(cleandata)\n",
    "airlineTweets[\"text\"] = airlineTweets[\"text\"].map(cleandata)\n",
    "\n",
    "genericTweets.to_csv(\"generic_tweets_clean.csv\")\n",
    "airlineTweets.to_csv(\"US_airline_tweets_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atUser = pd.DataFrame(columns=['atUser'])\n",
    "hashTag = pd.DataFrame(columns=['hashTag'])\n",
    "\n",
    "#Method to find regex pattern - used for finding \"@\"s and \"#\"s\n",
    "def getAtUsers(text, pattern):\n",
    "    return pattern.findall(text)\n",
    "\n",
    "#find airlines mentioned with \"@\"\n",
    "atUserRe = re.compile(r'@\\w+')\n",
    "for t in airlineTweets[\"text\"]:\n",
    "    for user in getAtUsers(t, atUserRe):\n",
    "        atUser = atUser.append({\"atUser\": user},ignore_index=True)\n",
    "        \n",
    "atAirCount = atUser.groupby([\"atUser\"]).size().reset_index(name='counts').sort_values(by=['counts'], ascending=False).head(20)\n",
    "atAirCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#atAirCount = atAirCount.set_index('atUser')\n",
    "aa = atAirCount.set_index(\"atUser\")\n",
    "aa.index.name=\"airline\"\n",
    "plot = aa.head(7).plot(kind=\"bar\",alpha=0.5, title=\"airline tweet count\")\n",
    "plot.set_ylabel(\"counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "genClean = airlineTweets = pd.read_csv(\"generic_tweets_clean.csv\")\n",
    "airClean = genericTweets = pd.read_csv(\"US_airline_tweets_clean.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11541, 7)\n",
      "(200000, 7)\n"
     ]
    }
   ],
   "source": [
    "def deepClean(text):\n",
    "    text = str(text)\n",
    "    #remove @\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    return text\n",
    "\n",
    "#airDeepClean = airClean.head(10000)\n",
    "#for c in range(airDeepClean.text.size):\n",
    "#    if airDeepClean.text[c]:\n",
    "#        airDeepClean.text[c] = deepClean(airDeepClean.text[c])\n",
    "\n",
    "#genDeepClean = genClean.head(10000)\n",
    "#for c in range(genDeepClean.text.size):\n",
    "#    genDeepClean.text[c] = deepClean(genDeepClean.text[c])\n",
    "print(airClean.shape)\n",
    "print(genClean.shape)\n",
    "cleandata = lambda x: deepClean(x)\n",
    "genClean[\"text\"] = genClean[\"text\"].map(cleandata)\n",
    "airClean[\"text\"] = airClean[\"text\"].map(cleandata)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7237666666666667\n",
      "Accuracy for C=0.05: 0.737\n",
      "Accuracy for C=0.25: 0.7421666666666666\n",
      "Accuracy for C=0.5: 0.7425\n",
      "Accuracy for C=1: 0.7403\n",
      "Final Accuracy: 0.6236028073823758\n",
      "('followfriday', 1.9673256401549069)\n",
      "('welcome', 1.9524867418965373)\n",
      "('yayy', 1.8481230527703165)\n",
      "('congratulations', 1.8480105618454274)\n",
      "('smiling', 1.828590532659728)\n",
      "('thankyou', 1.757862522452095)\n",
      "('smile', 1.7228350886004498)\n",
      "('blessed', 1.700541935593226)\n",
      "('pleasure', 1.6867064262637803)\n",
      "('refreshed', 1.6441721076018658)\n",
      "---------\n",
      "('sad', -3.1219590264864525)\n",
      "('sadly', -2.9062153656944667)\n",
      "('bummed', -2.6771705389101497)\n",
      "('mothers', -2.650108958004124)\n",
      "('gutted', -2.49139775091238)\n",
      "('poor', -2.4804936480328097)\n",
      "('lonely', -2.408432628846069)\n",
      "('missing', -2.379144617324849)\n",
      "('misses', -2.3257595316754407)\n",
      "('hates', -2.3145085727652175)\n"
     ]
    }
   ],
   "source": [
    "g_sample = genClean\n",
    "air_sample = airClean\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(g_sample.text)\n",
    "X = cv.transform(g_sample.text)\n",
    "target = g_sample[\"class\"]\n",
    "\n",
    "X_test = cv.transform(air_sample.text)\n",
    "X_test_target = [4 if s == \"possitive\" else 0 for s in air_sample.sentiment]\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target , test_size=0.3, random_state=42)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    lr = LogisticRegression(C=c, solver=\"saga\")\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "\n",
    "final_model = LogisticRegression(C=0.5)\n",
    "final_model.fit(X, target)\n",
    "\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(X_test_target, final_model.predict(X_test)))    \n",
    "    \n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:10]:\n",
    "    print (best_positive)\n",
    "\n",
    "print(\"---------\")\n",
    "\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:10]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9178, 7)\n",
      "(9178,)\n",
      "Final Accuracy: 0.6161946259985476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "862      9\n",
       "6818     7\n",
       "2670     3\n",
       "8380     1\n",
       "8738     1\n",
       "2956     3\n",
       "4159     3\n",
       "7647     3\n",
       "9672     1\n",
       "2473     5\n",
       "2552     3\n",
       "6006     7\n",
       "7203     3\n",
       "7644     3\n",
       "7890     3\n",
       "9440     1\n",
       "4669     3\n",
       "7810     3\n",
       "9418     3\n",
       "11375    1\n",
       "11384    3\n",
       "2309     7\n",
       "3485     3\n",
       "998      0\n",
       "10001    1\n",
       "10553    3\n",
       "6830     1\n",
       "6990     3\n",
       "5853     8\n",
       "8131     3\n",
       "        ..\n",
       "975      3\n",
       "8550     7\n",
       "10623    2\n",
       "382      3\n",
       "10693    2\n",
       "9831     7\n",
       "5100     8\n",
       "9175     3\n",
       "3804     3\n",
       "3525     1\n",
       "6687     7\n",
       "6896     1\n",
       "7837     3\n",
       "7705     5\n",
       "6821     7\n",
       "2959     2\n",
       "5601     0\n",
       "7188     2\n",
       "2443     1\n",
       "8902     6\n",
       "1918     1\n",
       "10600    8\n",
       "7307     8\n",
       "5562     3\n",
       "8865     3\n",
       "7332     7\n",
       "2065     6\n",
       "3939     1\n",
       "10065    3\n",
       "1174     4\n",
       "Name: negative_reason, Length: 2754, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfReasons = airClean.query('negative_reason == negative_reason').copy()\n",
    "#reasons = dfReasons.negative_reason.unique()\n",
    "print(dfReasons.shape)\n",
    "print(le.fit_transform(dfReasons[\"negative_reason\"]).shape)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "dfReasons[\"negative_reason\"] = le.fit_transform(dfReasons[\"negative_reason\"])\n",
    "\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(dfReasons.text)\n",
    "X = cv.transform(dfReasons.text)\n",
    "target = dfReasons.negative_reason\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target , test_size=0.3, random_state=42)\n",
    "\n",
    "final_model = LogisticRegression(C=0.5, solver='lbfgs',\n",
    "                         multi_class='multinomial')\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_val, final_model.predict(X_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
